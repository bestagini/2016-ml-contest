{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two folds OneVsRest RF classification with RF imputation of missing values\n",
    "\n",
    "### Contest entry by :[geoLEARN](http://geolearn.ca/)\n",
    "\n",
    "\n",
    "##### [Martin Blouin](https://github.com/mablou), [Lorenzo Perozzi](https://github.com/lperozzi), [Antoine Cat√©](https://github.com/Antoine-Cate) <br> \n",
    "in collaboration with  [Erwan Gloaguen](http://ete.inrs.ca/erwan-gloaguen)\n",
    "\n",
    "\n",
    "####  [Original contest notebook](../Facies_classification.ipynb) by Brendon Hall, [Enthought](https://www.enthought.com/)\n",
    "\n",
    "In this Notebook, a two-folds classification strategy is tested. Missing PE values in test data are imputed using RF Regressor. A OneVsRest Classifer using Random Forest is used as the model for both steps.\n",
    "The first step consists in predicting facies using the feature engineering method we proposed in our previous submission. The second steps takes the probability of occurence of each facies and further feature engineering using rolling windows on each well is performed before implmenting a second random forest to get the final prediction.\n",
    "Cross-validation testing suggests sligthly lower scores than our previous submission, but with a more stable prediction (less difference in success rate between wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Facies</th>\n",
       "      <th>Formation</th>\n",
       "      <th>Well Name</th>\n",
       "      <th>Depth</th>\n",
       "      <th>GR</th>\n",
       "      <th>ILD_log10</th>\n",
       "      <th>DeltaPHI</th>\n",
       "      <th>PHIND</th>\n",
       "      <th>PE</th>\n",
       "      <th>NM_M</th>\n",
       "      <th>RELPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2793.0</td>\n",
       "      <td>77.45</td>\n",
       "      <td>0.664</td>\n",
       "      <td>9.9</td>\n",
       "      <td>11.915</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2793.5</td>\n",
       "      <td>78.26</td>\n",
       "      <td>0.661</td>\n",
       "      <td>14.2</td>\n",
       "      <td>12.565</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2794.0</td>\n",
       "      <td>79.05</td>\n",
       "      <td>0.658</td>\n",
       "      <td>14.8</td>\n",
       "      <td>13.050</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2794.5</td>\n",
       "      <td>86.10</td>\n",
       "      <td>0.655</td>\n",
       "      <td>13.9</td>\n",
       "      <td>13.115</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2795.0</td>\n",
       "      <td>74.58</td>\n",
       "      <td>0.647</td>\n",
       "      <td>13.5</td>\n",
       "      <td>13.300</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Facies Formation  Well Name   Depth     GR  ILD_log10  DeltaPHI   PHIND  \\\n",
       "0       3     A1 SH  SHRIMPLIN  2793.0  77.45      0.664       9.9  11.915   \n",
       "1       3     A1 SH  SHRIMPLIN  2793.5  78.26      0.661      14.2  12.565   \n",
       "2       3     A1 SH  SHRIMPLIN  2794.0  79.05      0.658      14.8  13.050   \n",
       "3       3     A1 SH  SHRIMPLIN  2794.5  86.10      0.655      13.9  13.115   \n",
       "4       3     A1 SH  SHRIMPLIN  2795.0  74.58      0.647      13.5  13.300   \n",
       "\n",
       "    PE  NM_M  RELPOS  \n",
       "0  4.6     1   1.000  \n",
       "1  4.1     1   0.979  \n",
       "2  3.6     1   0.957  \n",
       "3  3.5     1   0.936  \n",
       "4  3.4     1   0.915  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### Importing all used packages\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib as mpl\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.colors as colors\n",
    "#from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "#import seaborn as sns\n",
    "\n",
    "from pandas import set_option\n",
    "# set_option(\"display.max_rows\", 10)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "###### Import packages needed for the make_vars functions\n",
    "import Feature_Engineering as FE\n",
    "\n",
    "##### import stuff from scikit learn\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score,LeavePGroupsOut, LeaveOneGroupOut, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, f1_score, accuracy_score, recall_score, precision_score\n",
    "\n",
    "filename = '../facies_vectors.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values (PE) imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####### create X_train and y_train\n",
    "X_train = df[['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'NM_M', 'RELPOS']][df.PE.notnull()]\n",
    "y_train = df['PE'][df.PE.notnull()]\n",
    "groups_train = df['Well Name'][df.PE.notnull()]\n",
    "\n",
    "X_fit = df[['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'NM_M', 'RELPOS']][df.PE.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cl = RandomForestRegressor(n_estimators=100)\n",
    "Cl.fit(X_train, y_train)\n",
    "y_predict = Cl.predict(X_fit)\n",
    "df['PE'][df.PE.isnull()] = y_predict\n",
    "training_data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "As stated in our [previous submission](Submission_increased_variance.ipynb), we believe that feature engineering has a high potential for increasing classification success. A strategy for building new variables is explained below.\n",
    "\n",
    "The dataset is distributed along a series of drillholes intersecting a stratigraphic sequence. Sedimentary facies tend to be deposited in sequences that reflect the evolution of the paleo-environment (variations in water depth, water temperature, biological activity, currents strenght, detrital input, ...). Each facies represents a specific depositional environment and is in contact with facies that represent a progressive transition to an other environment.\n",
    "Thus, there is a relationship between neighbouring samples, and the distribution of the data along drillholes can be as important as data values for predicting facies.\n",
    "\n",
    "A series of new variables (features) are calculated and tested below to help represent the relationship of neighbouring samples and the overall texture of the data along drillholes. These variables are:\n",
    "\n",
    "- detail and approximation coeficients at various levels of two [wavelet transforms](https://en.wikipedia.org/wiki/Discrete_wavelet_transform) (using two types of [Daubechies](https://en.wikipedia.org/wiki/Daubechies_wavelet) wavelets);\n",
    "- measures of the local entropy with variable observation windows;\n",
    "- measures of the local gradient with variable observation windows;\n",
    "- rolling statistical calculations (i.e., mean, standard deviation, min and max) with variable observation windows;\n",
    "- ratios between marine and non-marine lithofacies with different observation windows;\n",
    "- distances from the nearest marine or non-marine occurence uphole and downhole.\n",
    "\n",
    "Functions used to build these variables are located in the [Feature Engineering](Feature_Engineering.py) python script.\n",
    "\n",
    "All the data exploration work related to the conception and study of these variables is not presented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### cD From wavelet db1\n",
    "dwt_db1_cD_df = FE.make_dwt_vars_cD(wells_df=training_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                       levels=[1, 2, 3, 4], wavelet='db1')\n",
    "\n",
    "##### cA From wavelet db1\n",
    "dwt_db1_cA_df = FE.make_dwt_vars_cA(wells_df=training_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                       levels=[1, 2, 3, 4], wavelet='db1')\n",
    "\n",
    "##### cD From wavelet db3\n",
    "dwt_db3_cD_df = FE.make_dwt_vars_cD(wells_df=training_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                       levels=[1, 2, 3, 4], wavelet='db3')\n",
    "\n",
    "##### cA From wavelet db3\n",
    "dwt_db3_cA_df = FE.make_dwt_vars_cA(wells_df=training_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                       levels=[1, 2, 3, 4], wavelet='db3')\n",
    "\n",
    "##### From entropy\n",
    "entropy_df = FE.make_entropy_vars(wells_df=training_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                               l_foots=[2, 3, 4, 5, 7, 10])\n",
    "\n",
    "###### From gradient\n",
    "gradient_df = FE.make_gradient_vars(wells_df=training_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                                 dx_list=[2, 3, 4, 5, 6, 10, 20])\n",
    "\n",
    "##### From rolling average\n",
    "moving_av_df = FE.make_moving_av_vars(wells_df=training_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                                   windows=[1, 2, 5, 10, 20])\n",
    "\n",
    "##### From rolling standard deviation\n",
    "moving_std_df = FE.make_moving_std_vars(wells_df=training_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                                     windows=[3 , 4, 5, 7, 10, 15, 20])\n",
    "\n",
    "##### From rolling max\n",
    "moving_max_df = FE.make_moving_max_vars(wells_df=training_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                                     windows=[3, 4, 5, 7, 10, 15, 20])\n",
    "\n",
    "##### From rolling min\n",
    "moving_min_df = FE.make_moving_min_vars(wells_df=training_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                                     windows=[3 , 4, 5, 7, 10, 15, 20])\n",
    "\n",
    "###### From rolling NM/M ratio\n",
    "rolling_marine_ratio_df = FE.make_rolling_marine_ratio_vars(wells_df=training_data, windows=[5, 10, 15, 20, 30, 50, 75, 100, 200])\n",
    "\n",
    "###### From distance to NM and M, up and down\n",
    "dist_M_up_df = FE.make_distance_to_M_up_vars(wells_df=training_data)\n",
    "dist_M_down_df = FE.make_distance_to_M_down_vars(wells_df=training_data)\n",
    "dist_NM_up_df = FE.make_distance_to_NM_up_vars(wells_df=training_data)\n",
    "dist_NM_down_df = FE.make_distance_to_NM_down_vars(wells_df=training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_df_var = [dwt_db1_cD_df, dwt_db1_cA_df, dwt_db3_cD_df, dwt_db3_cA_df,\n",
    "               entropy_df, gradient_df, moving_av_df, moving_std_df, moving_max_df, moving_min_df,\n",
    "              rolling_marine_ratio_df, dist_M_up_df, dist_M_down_df, dist_NM_up_df, dist_NM_down_df]\n",
    "combined_df = training_data\n",
    "for var_df in list_df_var:\n",
    "    temp_df = var_df\n",
    "    combined_df = pd.concat([combined_df,temp_df],axis=1)\n",
    "combined_df.replace(to_replace=np.nan, value='-1', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the first-fold classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = combined_df.iloc[:, 4:]\n",
    "y = combined_df['Facies']\n",
    "groups = combined_df['Well Name']\n",
    "RF = RandomForestClassifier(n_estimators=100, max_features=0.1, min_samples_leaf=25,\n",
    "                            min_samples_split=50, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "Cl = OneVsRestClassifier(RF, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset for the second-fold classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### Doing a first prediction of lithologies using predict_proba\n",
    "cv=LeaveOneGroupOut().split(X, y, groups)\n",
    "RF = RandomForestClassifier(n_estimators=100, max_features=0.1, min_samples_leaf=25,\n",
    "                            min_samples_split=50, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "Cl = OneVsRestClassifier(RF, n_jobs=-1)\n",
    "proba = cross_val_predict(Cl, X, y, cv=cv, method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### creating new variables from predict proba\n",
    "list_facies = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
    "proba = pd.DataFrame(proba, columns=list_facies)\n",
    "proba = pd.concat([combined_df.iloc[:, :4], proba], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### From gradient\n",
    "gradient_df = FE.make_gradient_vars(wells_df=proba, logs=list_facies, dx_list=[2, 3, 4, 5, 6, 10, 20])\n",
    "\n",
    "##### From rolling average\n",
    "moving_av_df = FE.make_moving_av_vars(wells_df=proba, logs=list_facies, windows=[1, 2, 5, 10, 20])\n",
    "\n",
    "##### From rolling standard deviation\n",
    "moving_std_df = FE.make_moving_std_vars(wells_df=proba, logs=list_facies, windows=[3 , 4, 5, 7, 10, 15, 20])\n",
    "\n",
    "##### From rolling max\n",
    "moving_max_df = FE.make_moving_max_vars(wells_df=proba, logs=list_facies, windows=[3, 4, 5, 7, 10, 15, 20])\n",
    "\n",
    "##### From rolling min\n",
    "moving_min_df = FE.make_moving_min_vars(wells_df=proba, logs=list_facies, windows=[3 , 4, 5, 7, 10, 15, 20])\n",
    "\n",
    "list_df_var = [gradient_df, moving_av_df, moving_std_df, moving_max_df, moving_min_df]\n",
    "combined_df = proba\n",
    "for var_df in list_df_var:\n",
    "    combined_df = pd.concat([combined_df,var_df],axis=1)\n",
    "combined_df.replace(to_replace=np.nan, value='-1', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X2 = combined_df.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the second-fold classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cl2 = RandomForestClassifier(n_estimators=100, max_features=0.7, min_samples_leaf=0.01,\n",
    "                            min_samples_split=100, class_weight='balanced', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Formation</th>\n",
       "      <th>Well Name</th>\n",
       "      <th>Depth</th>\n",
       "      <th>GR</th>\n",
       "      <th>ILD_log10</th>\n",
       "      <th>DeltaPHI</th>\n",
       "      <th>PHIND</th>\n",
       "      <th>PE</th>\n",
       "      <th>NM_M</th>\n",
       "      <th>RELPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1 SH</td>\n",
       "      <td>STUART</td>\n",
       "      <td>2808.0</td>\n",
       "      <td>66.276</td>\n",
       "      <td>0.630</td>\n",
       "      <td>3.3</td>\n",
       "      <td>10.65</td>\n",
       "      <td>3.591</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1 SH</td>\n",
       "      <td>STUART</td>\n",
       "      <td>2808.5</td>\n",
       "      <td>77.252</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.5</td>\n",
       "      <td>11.95</td>\n",
       "      <td>3.341</td>\n",
       "      <td>1</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1 SH</td>\n",
       "      <td>STUART</td>\n",
       "      <td>2809.0</td>\n",
       "      <td>82.899</td>\n",
       "      <td>0.566</td>\n",
       "      <td>9.4</td>\n",
       "      <td>13.60</td>\n",
       "      <td>3.064</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1 SH</td>\n",
       "      <td>STUART</td>\n",
       "      <td>2809.5</td>\n",
       "      <td>80.671</td>\n",
       "      <td>0.593</td>\n",
       "      <td>9.5</td>\n",
       "      <td>13.25</td>\n",
       "      <td>2.977</td>\n",
       "      <td>1</td>\n",
       "      <td>0.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1 SH</td>\n",
       "      <td>STUART</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>75.971</td>\n",
       "      <td>0.638</td>\n",
       "      <td>8.7</td>\n",
       "      <td>12.35</td>\n",
       "      <td>3.020</td>\n",
       "      <td>1</td>\n",
       "      <td>0.911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Formation Well Name   Depth      GR  ILD_log10  DeltaPHI  PHIND     PE  \\\n",
       "0     A1 SH    STUART  2808.0  66.276      0.630       3.3  10.65  3.591   \n",
       "1     A1 SH    STUART  2808.5  77.252      0.585       6.5  11.95  3.341   \n",
       "2     A1 SH    STUART  2809.0  82.899      0.566       9.4  13.60  3.064   \n",
       "3     A1 SH    STUART  2809.5  80.671      0.593       9.5  13.25  2.977   \n",
       "4     A1 SH    STUART  2810.0  75.971      0.638       8.7  12.35  3.020   \n",
       "\n",
       "   NM_M  RELPOS  \n",
       "0     1   1.000  \n",
       "1     1   0.978  \n",
       "2     1   0.956  \n",
       "3     1   0.933  \n",
       "4     1   0.911  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = '../validation_data_nofacies.csv'\n",
    "test_data = pd.read_csv(filename)\n",
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing first fold of classification on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### cD From wavelet db1\n",
    "dwt_db1_cD_df = FE.make_dwt_vars_cD(wells_df=test_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                                    levels=[1, 2, 3, 4], wavelet='db1')\n",
    "\n",
    "##### cA From wavelet db1\n",
    "dwt_db1_cA_df = FE.make_dwt_vars_cA(wells_df=test_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                       levels=[1, 2, 3, 4], wavelet='db1')\n",
    "\n",
    "##### cD From wavelet db3\n",
    "dwt_db3_cD_df = FE.make_dwt_vars_cD(wells_df=test_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                       levels=[1, 2, 3, 4], wavelet='db3')\n",
    "\n",
    "##### cA From wavelet db3\n",
    "dwt_db3_cA_df = FE.make_dwt_vars_cA(wells_df=test_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                       levels=[1, 2, 3, 4], wavelet='db3')\n",
    "\n",
    "##### From entropy\n",
    "entropy_df = FE.make_entropy_vars(wells_df=test_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                               l_foots=[2, 3, 4, 5, 7, 10])\n",
    "\n",
    "###### From gradient\n",
    "gradient_df = FE.make_gradient_vars(wells_df=test_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                                 dx_list=[2, 3, 4, 5, 6, 10, 20])\n",
    "\n",
    "##### From rolling average\n",
    "moving_av_df = FE.make_moving_av_vars(wells_df=test_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                                   windows=[1, 2, 5, 10, 20])\n",
    "\n",
    "##### From rolling standard deviation\n",
    "moving_std_df = FE.make_moving_std_vars(wells_df=test_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                                     windows=[3 , 4, 5, 7, 10, 15, 20])\n",
    "\n",
    "##### From rolling max\n",
    "moving_max_df = FE.make_moving_max_vars(wells_df=test_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                                     windows=[3, 4, 5, 7, 10, 15, 20])\n",
    "\n",
    "##### From rolling min\n",
    "moving_min_df = FE.make_moving_min_vars(wells_df=test_data, logs=['GR', 'ILD_log10', 'DeltaPHI', 'PE', 'PHIND'],\n",
    "                                     windows=[3 , 4, 5, 7, 10, 15, 20])\n",
    "\n",
    "###### From rolling NM/M ratio\n",
    "rolling_marine_ratio_df = FE.make_rolling_marine_ratio_vars(wells_df=test_data, windows=[5, 10, 15, 20, 30, 50, 75, 100, 200])\n",
    "\n",
    "###### From distance to NM and M, up and down\n",
    "dist_M_up_df = FE.make_distance_to_M_up_vars(wells_df=test_data)\n",
    "dist_M_down_df = FE.make_distance_to_M_down_vars(wells_df=test_data)\n",
    "dist_NM_up_df = FE.make_distance_to_NM_up_vars(wells_df=test_data)\n",
    "dist_NM_down_df = FE.make_distance_to_NM_down_vars(wells_df=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_test_df = test_data\n",
    "list_df_var = [dwt_db1_cD_df, dwt_db1_cA_df, dwt_db3_cD_df, dwt_db3_cA_df,\n",
    "               entropy_df, gradient_df, moving_av_df, moving_std_df, moving_max_df, moving_min_df,\n",
    "              rolling_marine_ratio_df, dist_M_up_df, dist_M_down_df, dist_NM_up_df, dist_NM_down_df]\n",
    "for var_df in list_df_var:\n",
    "    temp_df = var_df\n",
    "    combined_test_df = pd.concat([combined_test_df,temp_df],axis=1)\n",
    "combined_test_df.replace(to_replace=np.nan, value='-1', inplace=True)\n",
    "\n",
    "X_test = combined_test_df.iloc[:, 3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cl.fit(X, y)\n",
    "proba_test = Cl.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the second fold of classification on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### creating new variables from predict proba\n",
    "list_facies = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
    "proba_test = pd.DataFrame(proba_test, columns=list_facies)\n",
    "proba_test = pd.concat([combined_test_df.iloc[:, :3], proba_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### From gradient\n",
    "gradient_df = FE.make_gradient_vars(wells_df=proba_test, logs=list_facies, dx_list=[2, 3, 4, 5, 6, 10, 20])\n",
    "\n",
    "##### From rolling average\n",
    "moving_av_df = FE.make_moving_av_vars(wells_df=proba_test, logs=list_facies, windows=[1, 2, 5, 10, 20])\n",
    "\n",
    "##### From rolling standard deviation\n",
    "moving_std_df = FE.make_moving_std_vars(wells_df=proba_test, logs=list_facies, windows=[3 , 4, 5, 7, 10, 15, 20])\n",
    "\n",
    "##### From rolling max\n",
    "moving_max_df = FE.make_moving_max_vars(wells_df=proba_test, logs=list_facies, windows=[3, 4, 5, 7, 10, 15, 20])\n",
    "\n",
    "##### From rolling min\n",
    "moving_min_df = FE.make_moving_min_vars(wells_df=proba_test, logs=list_facies, windows=[3 , 4, 5, 7, 10, 15, 20])\n",
    "\n",
    "list_df_var = [gradient_df, moving_av_df, moving_std_df, moving_max_df, moving_min_df]\n",
    "combined_test_df = proba_test\n",
    "for var_df in list_df_var:\n",
    "    combined_test_df = pd.concat([combined_test_df,var_df],axis=1)\n",
    "combined_test_df.replace(to_replace=np.nan, value='-1', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(830, 102)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well Name</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Predicted Facies #0</th>\n",
       "      <th>Predicted Facies #1</th>\n",
       "      <th>Predicted Facies #2</th>\n",
       "      <th>Predicted Facies #3</th>\n",
       "      <th>Predicted Facies #4</th>\n",
       "      <th>Predicted Facies #5</th>\n",
       "      <th>Predicted Facies #6</th>\n",
       "      <th>Predicted Facies #7</th>\n",
       "      <th>...</th>\n",
       "      <th>Predicted Facies #90</th>\n",
       "      <th>Predicted Facies #91</th>\n",
       "      <th>Predicted Facies #92</th>\n",
       "      <th>Predicted Facies #93</th>\n",
       "      <th>Predicted Facies #94</th>\n",
       "      <th>Predicted Facies #95</th>\n",
       "      <th>Predicted Facies #96</th>\n",
       "      <th>Predicted Facies #97</th>\n",
       "      <th>Predicted Facies #98</th>\n",
       "      <th>Predicted Facies #99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>STUART</td>\n",
       "      <td>2808.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STUART</td>\n",
       "      <td>2808.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STUART</td>\n",
       "      <td>2809.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STUART</td>\n",
       "      <td>2809.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>STUART</td>\n",
       "      <td>2810.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Well Name   Depth  Predicted Facies #0  Predicted Facies #1  \\\n",
       "0    STUART  2808.0                    2                    1   \n",
       "1    STUART  2808.5                    2                    2   \n",
       "2    STUART  2809.0                    2                    2   \n",
       "3    STUART  2809.5                    2                    2   \n",
       "4    STUART  2810.0                    2                    2   \n",
       "\n",
       "   Predicted Facies #2  Predicted Facies #3  Predicted Facies #4  \\\n",
       "0                    1                    1                    1   \n",
       "1                    2                    2                    2   \n",
       "2                    2                    2                    2   \n",
       "3                    2                    2                    2   \n",
       "4                    2                    2                    2   \n",
       "\n",
       "   Predicted Facies #5  Predicted Facies #6  Predicted Facies #7  \\\n",
       "0                    2                    1                    1   \n",
       "1                    2                    2                    2   \n",
       "2                    2                    2                    2   \n",
       "3                    2                    2                    2   \n",
       "4                    2                    2                    2   \n",
       "\n",
       "           ...           Predicted Facies #90  Predicted Facies #91  \\\n",
       "0          ...                              1                     1   \n",
       "1          ...                              2                     2   \n",
       "2          ...                              2                     2   \n",
       "3          ...                              2                     2   \n",
       "4          ...                              2                     2   \n",
       "\n",
       "   Predicted Facies #92  Predicted Facies #93  Predicted Facies #94  \\\n",
       "0                     1                     1                     1   \n",
       "1                     2                     2                     2   \n",
       "2                     2                     2                     2   \n",
       "3                     2                     2                     2   \n",
       "4                     2                     2                     2   \n",
       "\n",
       "   Predicted Facies #95  Predicted Facies #96  Predicted Facies #97  \\\n",
       "0                     1                     1                     1   \n",
       "1                     2                     2                     2   \n",
       "2                     2                     2                     2   \n",
       "3                     2                     2                     2   \n",
       "4                     2                     2                     2   \n",
       "\n",
       "   Predicted Facies #98  Predicted Facies #99  \n",
       "0                     1                     1  \n",
       "1                     2                     2  \n",
       "2                     2                     2  \n",
       "3                     2                     2  \n",
       "4                     2                     2  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test2 = combined_test_df.iloc[:, 3:]\n",
    "test_pred_df = combined_test_df[['Well Name', 'Depth']]\n",
    "for i in range(100):\n",
    "    Cl2.fit(X2, y)\n",
    "    y_test = Cl2.predict(X_test2)\n",
    "    y_test = pd.DataFrame(y_test, columns=['Predicted Facies #' + str(i)])\n",
    "    test_pred_df = pd.concat([test_pred_df, y_test], axis=1)\n",
    "print (test_pred_df.shape)\n",
    "test_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred_df.to_pickle('Prediction_submission4_TwoSteps.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
