{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title, authors, credit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "import classification_utilities as utils\n",
    "from ConvNet import inception_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FROM_SCRATCH = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Facies</th>\n",
       "      <th>Well Name</th>\n",
       "      <th>Depth</th>\n",
       "      <th>GR</th>\n",
       "      <th>ILD_log10</th>\n",
       "      <th>DeltaPHI</th>\n",
       "      <th>PHIND</th>\n",
       "      <th>PE</th>\n",
       "      <th>NM_M</th>\n",
       "      <th>RELPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2793.00</td>\n",
       "      <td>77.45</td>\n",
       "      <td>0.66</td>\n",
       "      <td>9.90</td>\n",
       "      <td>11.91</td>\n",
       "      <td>4.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2793.50</td>\n",
       "      <td>78.26</td>\n",
       "      <td>0.66</td>\n",
       "      <td>14.20</td>\n",
       "      <td>12.56</td>\n",
       "      <td>4.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4147</th>\n",
       "      <td>5</td>\n",
       "      <td>CHURCHMAN BIBLE</td>\n",
       "      <td>3122.00</td>\n",
       "      <td>51.47</td>\n",
       "      <td>0.96</td>\n",
       "      <td>3.08</td>\n",
       "      <td>7.71</td>\n",
       "      <td>3.15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4148</th>\n",
       "      <td>5</td>\n",
       "      <td>CHURCHMAN BIBLE</td>\n",
       "      <td>3122.50</td>\n",
       "      <td>50.03</td>\n",
       "      <td>0.97</td>\n",
       "      <td>2.61</td>\n",
       "      <td>6.67</td>\n",
       "      <td>3.29</td>\n",
       "      <td>2</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4149 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Facies        Well Name   Depth    GR  ILD_log10  DeltaPHI  PHIND   PE  \\\n",
       "0          3        SHRIMPLIN 2793.00 77.45       0.66      9.90  11.91 4.60   \n",
       "1          3        SHRIMPLIN 2793.50 78.26       0.66     14.20  12.56 4.10   \n",
       "...      ...              ...     ...   ...        ...       ...    ...  ...   \n",
       "4147       5  CHURCHMAN BIBLE 3122.00 51.47       0.96      3.08   7.71 3.15   \n",
       "4148       5  CHURCHMAN BIBLE 3122.50 50.03       0.97      2.61   6.67 3.29   \n",
       "\n",
       "      NM_M  RELPOS  \n",
       "0        1    1.00  \n",
       "1        1    0.98  \n",
       "...    ...     ...  \n",
       "4147     2    0.66  \n",
       "4148     2    0.65  \n",
       "\n",
       "[4149 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some parameters\n",
    "#feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS', 'Depth']\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "facies_names = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
    "facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00', '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']\n",
    "approximate_neighbors = [[2], [1, 3], [2], [5], [4, 6], [5, 7, 8], [6, 8, 9], [6, 7, 9], [7, 8]]\n",
    "\n",
    "# Load data from file\n",
    "training_data = pd.read_csv('../facies_vectors.csv').drop('Formation', axis=1)\n",
    "pd.set_option(\"display.max_rows\", 4, \"float_format\", lambda v: \"%.2f\" % v )\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4149, 7)\n",
      "(4149,)\n"
     ]
    }
   ],
   "source": [
    "# Well labels\n",
    "well_name = training_data['Well Name']\n",
    "\n",
    "# Features and labels\n",
    "X = training_data[feature_names].values\n",
    "y = training_data['Facies'].values\n",
    "print(X.shape)  # features\n",
    "print(y.shape)  # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not sure if good idea to use F9 well?\n",
    "well_to_be_used = []\n",
    "for well in well_name.unique():\n",
    "    if well != 'Recruit F9':\n",
    "        well_to_be_used.append(well)\n",
    "random.shuffle(well_to_be_used)        \n",
    "#well_to_be_used.append('Recruit F9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if FROM_SCRATCH is True:\n",
    "    # Fit KernelRidge with parameter selection based on 5-fold cross validation\n",
    "    krr_param_grid = {\"alpha\": [1e0, 1e-1, 1e-2, 1e-3],\n",
    "                      \"kernel\": ['laplacian'],\n",
    "                      \"gamma\": np.linspace(0.01,1,10)}\n",
    "    krr = GridSearchCV(KernelRidge(), cv=5, param_grid=krr_param_grid)\n",
    "\n",
    "    data_missing_vals = training_data[feature_names].copy()\n",
    "    data_no_missing_vals = data_missing_vals.dropna(axis = 0, inplace=False)\n",
    "    f_ = data_no_missing_vals.loc[:, data_no_missing_vals.columns != 'PE']\n",
    "    l_ = data_no_missing_vals.loc[:, 'PE']\n",
    "    krr.fit(f_, l_)\n",
    "    X[np.array(data_missing_vals.PE.isnull()),\n",
    "      4] = krr.predict(data_missing_vals.loc[data_missing_vals.PE.isnull(),:].drop('PE',axis=1,inplace=False))\n",
    "else:\n",
    "    del X\n",
    "    X = np.load('X_after_krr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data_imp = pd.DataFrame(X,columns=feature_names, index=well_name)\n",
    "training_data_imp['Facies'] = training_data['Facies'].values\n",
    "training_data_imp['Depth'] = training_data['Depth'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4149, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_step = 10\n",
    "\n",
    "batch_size = 32 \n",
    "training_epochs = 71#71\n",
    "\n",
    "sequence_length = 31\n",
    "perc_overlap = 30\n",
    "perturbation_ratio = 0.08\n",
    "\n",
    "initial_learning_rate = 0.001 \n",
    "epoch_decrease_every = 25\n",
    "decrease_factor = 0.96\n",
    "\n",
    "beta = .02#0.02\n",
    "clip_norm=1.5\n",
    "dropout_fc = 0.5 # probability to keep units in fully connected layers\n",
    "dropout_conv = 0.5 # probability to keep units in fully conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEWBY\n",
      "random realisation #1\n",
      "Epoch: 0001 , Minibatch Loss=  1.506495 , Training Accuracy=  0.42085\n",
      "Epoch: 0011 , Minibatch Loss=  1.048651 , Training Accuracy=  0.62162\n",
      "Epoch: 0021 , Minibatch Loss=  0.928421 , Training Accuracy=  0.68919\n",
      "Epoch: 0031 , Minibatch Loss=  0.887138 , Training Accuracy=  0.72780\n",
      "Epoch: 0041 , Minibatch Loss=  0.836614 , Training Accuracy=  0.73745\n",
      "Epoch: 0051 , Minibatch Loss=  0.843847 , Training Accuracy=  0.72973\n",
      "Epoch: 0061 , Minibatch Loss=  0.807839 , Training Accuracy=  0.74517\n",
      "Epoch: 0071 , Minibatch Loss=  0.817065 , Training Accuracy=  0.74710\n",
      "random realisation #2\n",
      "Epoch: 0001 , Minibatch Loss=  1.438035 , Training Accuracy=  0.45174\n",
      "Epoch: 0011 , Minibatch Loss=  1.051913 , Training Accuracy=  0.65058\n",
      "Epoch: 0021 , Minibatch Loss=  0.939903 , Training Accuracy=  0.68533\n",
      "Epoch: 0031 , Minibatch Loss=  0.917730 , Training Accuracy=  0.69305\n",
      "Epoch: 0041 , Minibatch Loss=  0.905735 , Training Accuracy=  0.67375\n",
      "Epoch: 0051 , Minibatch Loss=  0.840623 , Training Accuracy=  0.70270\n",
      "Epoch: 0061 , Minibatch Loss=  0.834898 , Training Accuracy=  0.72201\n",
      "Epoch: 0071 , Minibatch Loss=  0.798336 , Training Accuracy=  0.71429\n",
      "random realisation #3\n",
      "Epoch: 0001 , Minibatch Loss=  1.450656 , Training Accuracy=  0.48069\n",
      "Epoch: 0011 , Minibatch Loss=  1.019614 , Training Accuracy=  0.62355\n",
      "Epoch: 0021 , Minibatch Loss=  0.979476 , Training Accuracy=  0.65830\n",
      "Epoch: 0031 , Minibatch Loss=  0.916164 , Training Accuracy=  0.66409\n",
      "Epoch: 0041 , Minibatch Loss=  0.879585 , Training Accuracy=  0.70077\n",
      "Epoch: 0051 , Minibatch Loss=  0.893836 , Training Accuracy=  0.67954\n",
      "Epoch: 0061 , Minibatch Loss=  0.803441 , Training Accuracy=  0.73166\n",
      "Epoch: 0071 , Minibatch Loss=  0.834823 , Training Accuracy=  0.72008\n"
     ]
    }
   ],
   "source": [
    "n_runs = 3\n",
    "\n",
    "f1_blind = {}\n",
    "# loop over blind well\n",
    "for blind_well in ['NEWBY']:#well_to_be_used:\n",
    "    print(blind_well)\n",
    "    f1_blind[blind_well] = []\n",
    "    for i in range(n_runs):\n",
    "        tf.reset_default_graph()\n",
    "        print('random realisation #%d' % (i+1))\n",
    "        # training wells\n",
    "        wells_for_training = [well for well in well_to_be_used if well != blind_well]\n",
    "    \n",
    "\n",
    "\n",
    "        # Generate sequences\n",
    "        TST_X, TST_Y, scaler = utils.generate_sequences(training_data_imp, wells_for_training,feature_names,\n",
    "                                            sequence_length=sequence_length,perc_overlap=perc_overlap,\n",
    "                                            flip_ud=False)\n",
    "\n",
    "        Y_blind = utils.generate_sequences(training_data_imp, [blind_well],feature_names,\n",
    "                                                sequence_length=sequence_length,perc_overlap=perc_overlap,\n",
    "                                                flip_ud=False)[1]\n",
    "\n",
    "        X_blind, Y_blind_absolute_idx_pos = utils.generate_sequences(training_data_imp, [blind_well],feature_names,\n",
    "                                                        sequence_length=sequence_length, \n",
    "                                                        with_labels=False,perc_overlap=perc_overlap,\n",
    "                                                        flip_ud=False, scaler=scaler)\n",
    "    \n",
    "        # Take some data from training set for intermediate testing\n",
    "        X_train, Y_train, X_test, Y_test = utils.split_training_testing(TST_X, TST_Y, percentage_for_testing=10)\n",
    "\n",
    "    \n",
    "    \n",
    "        total_batch = X_train.shape[0]//batch_size\n",
    "\n",
    "    \n",
    "        # Parameters\n",
    "        # learning rate\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        initial_learning_rate = initial_learning_rate \n",
    "        lr = tf.train.exponential_decay(initial_learning_rate, global_step, epoch_decrease_every*total_batch, \n",
    "                                        decrease_factor, staircase=True)\n",
    "\n",
    "        \n",
    "\n",
    "        # Network i/o\n",
    "        n_input = sequence_length \n",
    "        n_features = len(feature_names)\n",
    "        n_classes = len(facies_names)\n",
    "\n",
    "\n",
    "        x_net = tf.placeholder(tf.float32, [None, 1, n_input, n_features])\n",
    "        y_net = tf.placeholder(tf.float32, [None, n_classes])\n",
    "        keep_prob_fc = tf.placeholder(tf.float32) #dropout\n",
    "        keep_prob_conv = tf.placeholder(tf.float32)\n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    \n",
    "        # Construct model\n",
    "        pred = inception_net(x_net, keep_prob_fc, is_training, dropout_conv=keep_prob_conv, clip_norm=clip_norm)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        conv_weights = [var for var in tf.global_variables() if 'kernel' in var.name]\n",
    "        weights_constraint = tf.reduce_sum(beta*tf.pack([tf.nn.l2_loss(w) for w in conv_weights]))\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_net))\n",
    "        cost = tf.add(loss,weights_constraint)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost, global_step=global_step)\n",
    "\n",
    "        # Evaluate model\n",
    "        correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y_net, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        # Initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "    \n",
    "        # Launch the graph\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "    \n",
    "            # Training cycle\n",
    "            for epoch in range(training_epochs):\n",
    "                X_train, Y_train, X_test, Y_test = utils.split_training_testing(TST_X, TST_Y, \n",
    "                                                                                percentage_for_testing=10)\n",
    "                # shuffle training and testing set\n",
    "                idx_train = np.random.permutation(np.arange(X_train.shape[0]))\n",
    "                idx_test = np.random.permutation(np.arange(X_test.shape[0]))\n",
    "        \n",
    "                x_train = X_train[idx_train,...]\n",
    "                y_train = Y_train[idx_train]\n",
    "                x_test = X_test[idx_test,...]\n",
    "                y_test = Y_test[idx_test]\n",
    "        \n",
    "                # perturbe training labels\n",
    "                y_train = utils.perturbe_label(y_train, approximate_neighbors, perturbation_ratio=perturbation_ratio)\n",
    "        \n",
    "                # Loop over all batches\n",
    "                for i in range(total_batch):\n",
    "                    batch_idx = range(i*batch_size,(i+1)*batch_size)\n",
    "                    batch_x = x_train[batch_idx,...] \n",
    "                    batch_y = utils.dense_to_one_hot(y_train[batch_idx],n_classes)\n",
    "                    # Run optimization op (backprop)\n",
    "                    sess.run(optimizer, feed_dict={x_net: batch_x, \n",
    "                                           y_net: batch_y, \n",
    "                                           keep_prob_fc: dropout_fc,\n",
    "                                                   keep_prob_conv:dropout_conv,\n",
    "                                                  is_training:True})\n",
    "            \n",
    "                # Calculate loss and accuracy\n",
    "                if epoch % display_step == 0:\n",
    "                    #rand_idx = np.random.permutation(np.arange(x_test.shape[0]))[:batch_size]\n",
    "                    #batch_x_test = x_test[rand_idx,...]\n",
    "                    #batch_y_test = utils.dense_to_one_hot(y_test[rand_idx],n_classes)\n",
    "                    loss, acc = sess.run([cost, accuracy], feed_dict={x_net: x_test,\n",
    "                                                              y_net: utils.dense_to_one_hot(y_test,n_classes),\n",
    "                                                                      keep_prob_fc: 1.,\n",
    "                                                                      keep_prob_conv:1.,\n",
    "                                                                      is_training:False})\n",
    "                    print(\"Epoch:\", '%04d' % (epoch+1), \", Minibatch Loss= \" , \\\n",
    "                          \"{:.6f}\".format(loss) , \", Training Accuracy= \" , \\\n",
    "                          \"{:.5f}\".format(acc))\n",
    "\n",
    "\n",
    "            # Calculate accuracy for blind well\n",
    "            acc , y_blind_pred = sess.run([accuracy, pred], feed_dict={x_net: X_blind,\n",
    "                                          y_net: utils.dense_to_one_hot(Y_blind,n_classes),\n",
    "                                                                       keep_prob_fc: 1.,\n",
    "                                                                       keep_prob_conv:1.,\n",
    "                                                                       is_training:False})\n",
    "    \n",
    "            y_blind_pred_sm = sess.run(tf.nn.softmax(y_blind_pred))\n",
    "    \n",
    "        \n",
    "        y_blind_pred_final = utils.redundant_to_final(training_data_imp.ix[blind_well]['Facies'].shape[0],\n",
    "                                                      utils.one_hot_to_dense(y_blind_pred_sm),\n",
    "                                                      Y_blind_absolute_idx_pos)\n",
    "    \n",
    "        y_blind_truth = training_data_imp.ix[blind_well]['Facies'].values\n",
    "        f1_blind[blind_well].append(f1_score(y_blind_truth, y_blind_pred_final, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(651, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_blind_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of runs per well: 3\n",
      "\n",
      "\n",
      "NEWBY f1 scores: \n",
      "mean: 0.570194, std: 0.013773, median: 0.578834, min: 0.550756, max:0.580994\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Number of runs per well: %d' % n_runs)\n",
    "print('\\n')\n",
    "for well in f1_blind:\n",
    "    mean = np.mean(np.array(f1_blind[well]))\n",
    "    std = np.std(np.array(f1_blind[well]))\n",
    "    median = np.median(np.array(f1_blind[well]))\n",
    "    minimum = np.min(np.array(f1_blind[well]))\n",
    "    maximum = np.max(np.array(f1_blind[well]))                  \n",
    "    print('%s f1 scores: ' % well)\n",
    "    print('mean: %f, std: %f, median: %f, min: %f, max:%f' % (mean, std, median, minimum, maximum))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots()\n",
    "#ax.imshow(X_train[np.random.randint(X_train.shape[0]),0,:,:], aspect='auto')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
