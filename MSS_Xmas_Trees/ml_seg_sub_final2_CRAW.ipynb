{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contest entry by Wouter Kimman \n",
    "\n",
    "\n",
    "Strategy: \n",
    "----------------------------------------------\n",
    "stacking and optimized selective use of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.fft import rfft\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import pandas as pd\n",
    "import timeit\n",
    "from sqlalchemy.sql import text\n",
    "from sklearn import tree\n",
    "#from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "#import sherlock.filesystem as sfs\n",
    "#import sherlock.database as sdb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from collections import Counter, OrderedDict\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def permute_facies_nr(predicted_super, predicted0, faciesnr):\n",
    "    predicted=predicted0.copy()\n",
    "    N=len(predicted)\n",
    "    for ii in range(N):\n",
    "        if predicted_super[ii]==1:\n",
    "            predicted[ii]=faciesnr  \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarify(dataset0, facies_nr):\n",
    "    dataset=dataset0.copy()\n",
    "    mask=dataset != facies_nr\n",
    "    dataset[mask]=0\n",
    "    mask=dataset == facies_nr\n",
    "    dataset[mask]=1    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_balanced_binary(df_in, faciesnr, factor):\n",
    "    df=df_in.copy()\n",
    "    y=df['Facies'].values\n",
    "    y0=binarify(y, faciesnr)\n",
    "    df['Facies']=y0\n",
    "\n",
    "    df1=df[df['Facies']==1]\n",
    "    X_part1=df1.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "    y_part1=df1['Facies'].values\n",
    "    N1=len(df1)\n",
    "\n",
    "    df2=df[df['Facies']==0]\n",
    "    X_part0=df2.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "    y_part0=df2['Facies'].values\n",
    "    N2=len(df2)\n",
    "    print \"ratio now:\"\n",
    "    print float(N2)/float(N1)\n",
    "    ratio_to_keep=factor*float(N1)/float(N2)\n",
    "    print \"ratio after:\"\n",
    "    print float(N2)/(factor*float(N1))\n",
    "    dum1, X_part2, dum2, y_part2 = train_test_split(X_part0, y_part0, test_size=ratio_to_keep, random_state=42)\n",
    "\n",
    "    tmp=[X_part1, X_part2]  \n",
    "    X = pd.concat(tmp, axis=0)\n",
    "    y = np.concatenate((y_part1, y_part2))\n",
    "    return X, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def phaseI_model(regime_train, correctA, go_B, clf, pred_array, pred_blind, features_blind):      \n",
    "    clf.fit(regime_train,correctA)     \n",
    "    predicted_B = clf.predict(go_B)\n",
    "    pred_array = np.vstack((predicted_B, pred_array))   \n",
    "    predicted_blind1 = clf.predict(features_blind)\n",
    "    pred_blind = np.vstack((predicted_blind1, pred_blind))    \n",
    "    return pred_array, pred_blind\n",
    "\n",
    "def phaseI_model_scaled(regime_train, correctA, go_B, clf, pred_array, pred_blind, features_blind):   \n",
    "    regime_train=StandardScaler().fit_transform(regime_train)\n",
    "    go_B=StandardScaler().fit_transform(go_B)\n",
    "    features_blind=StandardScaler().fit_transform(features_blind)\n",
    "    clf.fit(regime_train,correctA)     \n",
    "    predicted_B = clf.predict(go_B)\n",
    "    pred_array = np.vstack((predicted_B, pred_array))\n",
    "    predicted_blind1 = clf.predict(features_blind)\n",
    "    pred_blind = np.vstack((predicted_blind1, pred_blind))\n",
    "    return pred_array, pred_blind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_structure_for_regimes(df):\n",
    "    allfeats=['GR','ILD_log10','DeltaPHI','PHIND','PE','NM_M','RELPOS']\n",
    "    data_all = []\n",
    "    for feat in allfeats:\n",
    "        dff=df.groupby('Well Name').describe(percentiles=[0.1, 0.25, .5, 0.75, 0.9]).reset_index().pivot(index='Well Name', values=feat, columns='level_1')\n",
    "        dff = dff.drop(['count'], axis=1)\n",
    "        cols=dff.columns\n",
    "        cols_new=[]\n",
    "        for ii in cols:\n",
    "            strin=feat + \"_\" + str(ii)\n",
    "            cols_new.append(strin)\n",
    "        dff.columns=cols_new \n",
    "        dff1=dff.reset_index()\n",
    "        if feat=='GR':\n",
    "            data_all.append(dff1)\n",
    "        else:\n",
    "            data_all.append(dff1.iloc[:,1:])\n",
    "    data_all = pd.concat(data_all,axis=1)\n",
    "    return data_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def magic(df):\n",
    "    df1=df.copy()\n",
    "    b, a = signal.butter(2, 0.2, btype='high', analog=False)\n",
    "    feats0=['GR','ILD_log10','DeltaPHI','PHIND','PE','NM_M','RELPOS']\n",
    "    #feats01=['GR','ILD_log10','DeltaPHI','PHIND']\n",
    "    #feats01=['DeltaPHI']\n",
    "    #feats01=['GR','DeltaPHI','PHIND']\n",
    "    feats01=['GR',]\n",
    "    feats02=['PHIND']\n",
    "    #feats02=[]\n",
    "    for ii in feats0:\n",
    "        df1[ii]=df[ii]\n",
    "        name1=ii + '_1'\n",
    "        name2=ii + '_2'\n",
    "        name3=ii + '_3'\n",
    "        name4=ii + '_4'\n",
    "        name5=ii + '_5'\n",
    "        name6=ii + '_6'\n",
    "        name7=ii + '_7'\n",
    "        name8=ii + '_8'\n",
    "        name9=ii + '_9'\n",
    "        xx1 = list(df[ii])\n",
    "        xx_mf= signal.medfilt(xx1,9)\n",
    "        x_min1=np.roll(xx_mf, 1)\n",
    "        x_min2=np.roll(xx_mf, -1)\n",
    "        x_min3=np.roll(xx_mf, 3)\n",
    "        x_min4=np.roll(xx_mf, 4)\n",
    "        xx1a=xx1-np.mean(xx1)\n",
    "        xx_fil = signal.filtfilt(b, a, xx1)        \n",
    "        xx_grad=np.gradient(xx1a) \n",
    "        x_min5=np.roll(xx_grad, 3)\n",
    "        #df1[name4]=xx_mf\n",
    "        if ii in feats01: \n",
    "            df1[name1]=x_min3\n",
    "            df1[name2]=xx_fil\n",
    "            df1[name3]=xx_grad\n",
    "            df1[name4]=xx_mf \n",
    "            df1[name5]=x_min1\n",
    "            df1[name6]=x_min2\n",
    "            df1[name7]=x_min4\n",
    "            #df1[name8]=x_min5\n",
    "            #df1[name9]=x_min2\n",
    "        if ii in feats02:\n",
    "            df1[name1]=x_min3\n",
    "            df1[name2]=xx_fil\n",
    "            df1[name3]=xx_grad\n",
    "            #df1[name4]=xx_mf \n",
    "            df1[name5]=x_min1\n",
    "            #df1[name6]=x_min2 \n",
    "            #df1[name7]=x_min4\n",
    "    return df1\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#As others have done, this is Paolo Bestagini's pre-preoccessing routine \n",
    "# Feature windows concatenation function\n",
    "def augment_features_window(X, N_neig):\n",
    "    \n",
    "    # Parameters\n",
    "    N_row = X.shape[0]\n",
    "    N_feat = X.shape[1]\n",
    "\n",
    "    # Zero padding\n",
    "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
    "\n",
    "    # Loop over windows\n",
    "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
    "    for r in np.arange(N_row)+N_neig:\n",
    "        this_row = []\n",
    "        for c in np.arange(-N_neig,N_neig+1):\n",
    "            this_row = np.hstack((this_row, X[r+c]))\n",
    "        X_aug[r-N_neig] = this_row\n",
    "\n",
    "    return X_aug\n",
    "\n",
    "\n",
    "# Feature gradient computation function\n",
    "def augment_features_gradient(X, depth):\n",
    "    \n",
    "    # Compute features gradient\n",
    "    d_diff = np.diff(depth).reshape((-1, 1))\n",
    "    d_diff[d_diff==0] = 0.001\n",
    "    X_diff = np.diff(X, axis=0)\n",
    "    X_grad = X_diff / d_diff\n",
    "        \n",
    "    # Compensate for last missing value\n",
    "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
    "    \n",
    "    return X_grad\n",
    "\n",
    "\n",
    "# Feature augmentation function\n",
    "def augment_features(X, well, depth, N_neig=1):\n",
    "    \n",
    "    # Augment features\n",
    "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
    "    for w in np.unique(well):\n",
    "        w_idx = np.where(well == w)[0]\n",
    "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
    "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
    "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
    "    \n",
    "    # Find padded rows\n",
    "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
    "    \n",
    "    return X_aug, padded_rows\n",
    "\n",
    "#X_aug, padded_rows = augment_features(X, well, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filename = 'training_data.csv'\n",
    "filename = 'facies_vectors.csv'\n",
    "training_data0 = pd.read_csv(filename)\n",
    "filename = 'validation_data_nofacies.csv'\n",
    "test_data = pd.read_csv(filename)\n",
    "\n",
    "#blindwell='CHURCHMAN BIBLE'\n",
    "#blindwell='LUKE G U'\n",
    "blindwell='CRAWFORD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SHRIMPLIN' 'ALEXANDER D' 'SHANKLE' 'LUKE G U' 'KIMZEY A' 'CROSS H CATTLE'\n",
      " 'NOLAN' 'Recruit F9' 'NEWBY' 'CHURCHMAN BIBLE']\n"
     ]
    }
   ],
   "source": [
    "all_wells=training_data0['Well Name'].unique()\n",
    "print all_wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5515\n",
      "SHRIMPLIN\n",
      "471\n",
      "using median of local\n",
      "ALEXANDER D\n",
      "466\n",
      "using median of total\n",
      "SHANKLE\n",
      "449\n",
      "using median of local\n",
      "LUKE G U\n",
      "461\n",
      "using median of local\n",
      "KIMZEY A\n",
      "439\n",
      "using median of total\n",
      "CROSS H CATTLE\n",
      "501\n",
      "using median of local\n",
      "NOLAN\n",
      "415\n",
      "using median of local\n",
      "Recruit F9\n",
      "80\n",
      "using median of local\n",
      "NEWBY\n",
      "463\n",
      "using median of local\n",
      "CHURCHMAN BIBLE\n",
      "404\n",
      "using median of local\n",
      "4149\n",
      "4149\n"
     ]
    }
   ],
   "source": [
    "# what to do with the naans\n",
    "training_data1=training_data0.copy()\n",
    "me_tot=training_data1['PE'].median()\n",
    "print me_tot\n",
    "for well in all_wells:\n",
    "    df=training_data0[training_data0['Well Name'] == well] \n",
    "    print well\n",
    "    print len(df)\n",
    "    df0=df.dropna()\n",
    "    #print len(df0)\n",
    "    if len(df0) > 0:\n",
    "        print \"using median of local\"\n",
    "        me=df['PE'].median()\n",
    "        df=df.fillna(value=me)\n",
    "    else:\n",
    "        print \"using median of total\"\n",
    "        df=df.fillna(value=me_tot)\n",
    "    training_data1[training_data0['Well Name'] == well] =df\n",
    "    \n",
    "\n",
    "print len(training_data1)\n",
    "df0=training_data1.dropna()\n",
    "print len(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4149\n",
      "4149\n",
      "4149\n",
      "4143\n"
     ]
    }
   ],
   "source": [
    "#remove outliers\n",
    "df=training_data1.copy()\n",
    "print len(df)\n",
    "df0=df.dropna()\n",
    "print len(df0)\n",
    "df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "#df=pd.DataFrame(np.random.randn(20,3))\n",
    "#df.iloc[3,2]=5\n",
    "print len(df1)\n",
    "df2=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "print len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2a=df2[df2['Well Name'] != 'Recruit F9'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_all=create_structure_for_regimes(df2a)\n",
    "data_test=create_structure_for_regimes(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well Name</th>\n",
       "      <th>GR_10%</th>\n",
       "      <th>GR_25%</th>\n",
       "      <th>GR_50%</th>\n",
       "      <th>GR_75%</th>\n",
       "      <th>GR_90%</th>\n",
       "      <th>GR_max</th>\n",
       "      <th>GR_mean</th>\n",
       "      <th>GR_min</th>\n",
       "      <th>GR_std</th>\n",
       "      <th>...</th>\n",
       "      <th>NM_M_std</th>\n",
       "      <th>RELPOS_10%</th>\n",
       "      <th>RELPOS_25%</th>\n",
       "      <th>RELPOS_50%</th>\n",
       "      <th>RELPOS_75%</th>\n",
       "      <th>RELPOS_90%</th>\n",
       "      <th>RELPOS_max</th>\n",
       "      <th>RELPOS_mean</th>\n",
       "      <th>RELPOS_min</th>\n",
       "      <th>RELPOS_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRAWFORD</td>\n",
       "      <td>24.9075</td>\n",
       "      <td>41.1465</td>\n",
       "      <td>58.413</td>\n",
       "      <td>73.55225</td>\n",
       "      <td>84.7215</td>\n",
       "      <td>167.803</td>\n",
       "      <td>58.666020</td>\n",
       "      <td>16.197</td>\n",
       "      <td>26.033591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438386</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>0.2775</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.7765</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.52623</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.287468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STUART</td>\n",
       "      <td>24.1612</td>\n",
       "      <td>34.5620</td>\n",
       "      <td>57.851</td>\n",
       "      <td>72.50275</td>\n",
       "      <td>82.7020</td>\n",
       "      <td>220.413</td>\n",
       "      <td>56.819901</td>\n",
       "      <td>12.036</td>\n",
       "      <td>28.600303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483098</td>\n",
       "      <td>0.1436</td>\n",
       "      <td>0.3180</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.7795</td>\n",
       "      <td>0.9257</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.54300</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.279796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Well Name   GR_10%   GR_25%  GR_50%    GR_75%   GR_90%   GR_max    GR_mean  \\\n",
       "0  CRAWFORD  24.9075  41.1465  58.413  73.55225  84.7215  167.803  58.666020   \n",
       "1    STUART  24.1612  34.5620  57.851  72.50275  82.7020  220.413  56.819901   \n",
       "\n",
       "   GR_min     GR_std     ...      NM_M_std  RELPOS_10%  RELPOS_25%  \\\n",
       "0  16.197  26.033591     ...      0.438386      0.1240      0.2775   \n",
       "1  12.036  28.600303     ...      0.483098      0.1436      0.3180   \n",
       "\n",
       "   RELPOS_50%  RELPOS_75%  RELPOS_90%  RELPOS_max  RELPOS_mean  RELPOS_min  \\\n",
       "0       0.537      0.7765      0.9145         1.0      0.52623       0.017   \n",
       "1       0.550      0.7795      0.9257         1.0      0.54300       0.013   \n",
       "\n",
       "   RELPOS_std  \n",
       "0    0.287468  \n",
       "1    0.279796  \n",
       "\n",
       "[2 rows x 64 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**kmeans clustering to find natural clusters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well Name</th>\n",
       "      <th>GR_10%</th>\n",
       "      <th>GR_25%</th>\n",
       "      <th>GR_50%</th>\n",
       "      <th>GR_75%</th>\n",
       "      <th>GR_90%</th>\n",
       "      <th>GR_max</th>\n",
       "      <th>GR_mean</th>\n",
       "      <th>GR_min</th>\n",
       "      <th>GR_std</th>\n",
       "      <th>...</th>\n",
       "      <th>NM_M_std</th>\n",
       "      <th>RELPOS_10%</th>\n",
       "      <th>RELPOS_25%</th>\n",
       "      <th>RELPOS_50%</th>\n",
       "      <th>RELPOS_75%</th>\n",
       "      <th>RELPOS_90%</th>\n",
       "      <th>RELPOS_max</th>\n",
       "      <th>RELPOS_mean</th>\n",
       "      <th>RELPOS_min</th>\n",
       "      <th>RELPOS_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALEXANDER D</td>\n",
       "      <td>25.9700</td>\n",
       "      <td>44.8650</td>\n",
       "      <td>72.285</td>\n",
       "      <td>88.92750</td>\n",
       "      <td>100.7850</td>\n",
       "      <td>168.430</td>\n",
       "      <td>68.280236</td>\n",
       "      <td>13.340</td>\n",
       "      <td>28.635640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497648</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.76125</td>\n",
       "      <td>0.9105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.515584</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.289040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHURCHMAN BIBLE</td>\n",
       "      <td>33.6532</td>\n",
       "      <td>41.3755</td>\n",
       "      <td>59.203</td>\n",
       "      <td>74.31250</td>\n",
       "      <td>95.6691</td>\n",
       "      <td>222.500</td>\n",
       "      <td>63.683453</td>\n",
       "      <td>14.469</td>\n",
       "      <td>33.333992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448185</td>\n",
       "      <td>0.1265</td>\n",
       "      <td>0.2860</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.80000</td>\n",
       "      <td>0.9290</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.544317</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.293647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CROSS H CATTLE</td>\n",
       "      <td>50.4050</td>\n",
       "      <td>60.1980</td>\n",
       "      <td>73.595</td>\n",
       "      <td>84.81900</td>\n",
       "      <td>94.6482</td>\n",
       "      <td>184.021</td>\n",
       "      <td>72.423513</td>\n",
       "      <td>13.893</td>\n",
       "      <td>20.851035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458631</td>\n",
       "      <td>0.1066</td>\n",
       "      <td>0.2580</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.75000</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.503288</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.288401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KIMZEY A</td>\n",
       "      <td>23.5474</td>\n",
       "      <td>34.8140</td>\n",
       "      <td>56.582</td>\n",
       "      <td>66.58800</td>\n",
       "      <td>73.8642</td>\n",
       "      <td>245.360</td>\n",
       "      <td>53.334339</td>\n",
       "      <td>10.149</td>\n",
       "      <td>24.761446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487142</td>\n",
       "      <td>0.1328</td>\n",
       "      <td>0.3040</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.78500</td>\n",
       "      <td>0.9202</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.539453</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.284105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LUKE G U</td>\n",
       "      <td>29.6800</td>\n",
       "      <td>48.4500</td>\n",
       "      <td>66.190</td>\n",
       "      <td>78.49000</td>\n",
       "      <td>87.3500</td>\n",
       "      <td>195.890</td>\n",
       "      <td>64.777223</td>\n",
       "      <td>13.530</td>\n",
       "      <td>27.427174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499410</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>0.2630</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.76100</td>\n",
       "      <td>0.9120</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.514095</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.286902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NEWBY</td>\n",
       "      <td>28.4880</td>\n",
       "      <td>38.1100</td>\n",
       "      <td>60.330</td>\n",
       "      <td>71.44500</td>\n",
       "      <td>85.7180</td>\n",
       "      <td>305.870</td>\n",
       "      <td>60.733045</td>\n",
       "      <td>13.920</td>\n",
       "      <td>33.620632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486991</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.2930</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.77150</td>\n",
       "      <td>0.9198</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.528138</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.285096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NOLAN</td>\n",
       "      <td>23.7500</td>\n",
       "      <td>43.3905</td>\n",
       "      <td>72.375</td>\n",
       "      <td>88.50000</td>\n",
       "      <td>102.5250</td>\n",
       "      <td>247.500</td>\n",
       "      <td>68.693940</td>\n",
       "      <td>13.250</td>\n",
       "      <td>32.730216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499834</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0.3070</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.79350</td>\n",
       "      <td>0.9206</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.549007</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.285563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SHANKLE</td>\n",
       "      <td>35.0760</td>\n",
       "      <td>54.9600</td>\n",
       "      <td>66.600</td>\n",
       "      <td>75.15000</td>\n",
       "      <td>83.1820</td>\n",
       "      <td>242.750</td>\n",
       "      <td>65.431180</td>\n",
       "      <td>18.400</td>\n",
       "      <td>25.696418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474493</td>\n",
       "      <td>0.1188</td>\n",
       "      <td>0.2580</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.74400</td>\n",
       "      <td>0.8952</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.503118</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.282082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>28.6580</td>\n",
       "      <td>46.8300</td>\n",
       "      <td>69.300</td>\n",
       "      <td>85.85000</td>\n",
       "      <td>96.3580</td>\n",
       "      <td>274.000</td>\n",
       "      <td>68.169936</td>\n",
       "      <td>13.280</td>\n",
       "      <td>32.170245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500342</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.76500</td>\n",
       "      <td>0.9154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.514399</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.290373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRAWFORD</td>\n",
       "      <td>24.9075</td>\n",
       "      <td>41.1465</td>\n",
       "      <td>58.413</td>\n",
       "      <td>73.55225</td>\n",
       "      <td>84.7215</td>\n",
       "      <td>167.803</td>\n",
       "      <td>58.666020</td>\n",
       "      <td>16.197</td>\n",
       "      <td>26.033591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438386</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>0.2775</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.77650</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.526230</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.287468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>STUART</td>\n",
       "      <td>24.1612</td>\n",
       "      <td>34.5620</td>\n",
       "      <td>57.851</td>\n",
       "      <td>72.50275</td>\n",
       "      <td>82.7020</td>\n",
       "      <td>220.413</td>\n",
       "      <td>56.819901</td>\n",
       "      <td>12.036</td>\n",
       "      <td>28.600303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483098</td>\n",
       "      <td>0.1436</td>\n",
       "      <td>0.3180</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.77950</td>\n",
       "      <td>0.9257</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.279796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Well Name   GR_10%   GR_25%  GR_50%    GR_75%    GR_90%   GR_max  \\\n",
       "0      ALEXANDER D  25.9700  44.8650  72.285  88.92750  100.7850  168.430   \n",
       "1  CHURCHMAN BIBLE  33.6532  41.3755  59.203  74.31250   95.6691  222.500   \n",
       "2   CROSS H CATTLE  50.4050  60.1980  73.595  84.81900   94.6482  184.021   \n",
       "3         KIMZEY A  23.5474  34.8140  56.582  66.58800   73.8642  245.360   \n",
       "4         LUKE G U  29.6800  48.4500  66.190  78.49000   87.3500  195.890   \n",
       "5            NEWBY  28.4880  38.1100  60.330  71.44500   85.7180  305.870   \n",
       "6            NOLAN  23.7500  43.3905  72.375  88.50000  102.5250  247.500   \n",
       "7          SHANKLE  35.0760  54.9600  66.600  75.15000   83.1820  242.750   \n",
       "8        SHRIMPLIN  28.6580  46.8300  69.300  85.85000   96.3580  274.000   \n",
       "0         CRAWFORD  24.9075  41.1465  58.413  73.55225   84.7215  167.803   \n",
       "1           STUART  24.1612  34.5620  57.851  72.50275   82.7020  220.413   \n",
       "\n",
       "     GR_mean  GR_min     GR_std     ...      NM_M_std  RELPOS_10%  RELPOS_25%  \\\n",
       "0  68.280236  13.340  28.635640     ...      0.497648      0.1125      0.2670   \n",
       "1  63.683453  14.469  33.333992     ...      0.448185      0.1265      0.2860   \n",
       "2  72.423513  13.893  20.851035     ...      0.458631      0.1066      0.2580   \n",
       "3  53.334339  10.149  24.761446     ...      0.487142      0.1328      0.3040   \n",
       "4  64.777223  13.530  27.427174     ...      0.499410      0.1190      0.2630   \n",
       "5  60.733045  13.920  33.620632     ...      0.486991      0.1234      0.2930   \n",
       "6  68.693940  13.250  32.730216     ...      0.499834      0.1336      0.3070   \n",
       "7  65.431180  18.400  25.696418     ...      0.474493      0.1188      0.2580   \n",
       "8  68.169936  13.280  32.170245     ...      0.500342      0.1110      0.2590   \n",
       "0  58.666020  16.197  26.033591     ...      0.438386      0.1240      0.2775   \n",
       "1  56.819901  12.036  28.600303     ...      0.483098      0.1436      0.3180   \n",
       "\n",
       "   RELPOS_50%  RELPOS_75%  RELPOS_90%  RELPOS_max  RELPOS_mean  RELPOS_min  \\\n",
       "0       0.518     0.76125      0.9105         1.0     0.515584       0.000   \n",
       "1       0.567     0.80000      0.9290         1.0     0.544317       0.011   \n",
       "2       0.500     0.75000      0.9000         1.0     0.503288       0.013   \n",
       "3       0.548     0.78500      0.9202         1.0     0.539453       0.010   \n",
       "4       0.522     0.76100      0.9120         1.0     0.514095       0.010   \n",
       "5       0.535     0.77150      0.9198         1.0     0.528138       0.010   \n",
       "6       0.577     0.79350      0.9206         1.0     0.549007       0.017   \n",
       "7       0.500     0.74400      0.8952         1.0     0.503118       0.010   \n",
       "8       0.511     0.76500      0.9154         1.0     0.514399       0.010   \n",
       "0       0.537     0.77650      0.9145         1.0     0.526230       0.017   \n",
       "1       0.550     0.77950      0.9257         1.0     0.543000       0.013   \n",
       "\n",
       "   RELPOS_std  \n",
       "0    0.289040  \n",
       "1    0.293647  \n",
       "2    0.288401  \n",
       "3    0.284105  \n",
       "4    0.286902  \n",
       "5    0.285096  \n",
       "6    0.285563  \n",
       "7    0.282082  \n",
       "8    0.290373  \n",
       "0    0.287468  \n",
       "1    0.279796  \n",
       "\n",
       "[11 rows x 64 columns]"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "frames = [data_all, data_test]\n",
    "X = pd.concat(frames)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = X.drop(['Well Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2).fit(X1)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Through experimenting with the cluster size I've decided on 4 clusters.\n",
    "This corresponds largely with the corresponding similarity in facies distribution\n",
    "\n",
    "**CRAWFORD is most similar to ALEXANDER and LUKE**. This will called cluster 1. (The only ones with facies 1)\n",
    "\n",
    "**STUART is most similar to KIMZEY and NOLAN** This will be called cluster 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collating the Data:\n",
    "-----------------------------------------------------------\n",
    "based on the regimes we determined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1428\n",
      "1707\n",
      "1014\n"
     ]
    }
   ],
   "source": [
    "# based on kmeans clustering\n",
    "data=[]\n",
    "df = training_data0[training_data0['Well Name'] == 'ALEXANDER D'] \n",
    "data.append(df)\n",
    "df = training_data0[training_data0['Well Name'] == 'LUKE G U']  \n",
    "data.append(df)\n",
    "df = training_data0[training_data0['Well Name'] == 'CROSS H CATTLE']  \n",
    "data.append(df)\n",
    "Regime_1 = pd.concat(data, axis=0)\n",
    "print len(Regime_1)\n",
    "\n",
    "data=[]\n",
    "df = training_data0[training_data0['Well Name'] == 'KIMZEY A']  \n",
    "data.append(df)\n",
    "df = training_data0[training_data0['Well Name'] == 'NOLAN']\n",
    "data.append(df)\n",
    "df = training_data0[training_data0['Well Name'] == 'CHURCHMAN BIBLE']  \n",
    "data.append(df)\n",
    "df = training_data0[training_data0['Well Name'] == 'SHANKLE'] \n",
    "data.append(df)\n",
    "Regime_2 = pd.concat(data, axis=0)\n",
    "print len(Regime_2)\n",
    "\n",
    "data=[]\n",
    "\n",
    "df = training_data0[training_data0['Well Name'] == 'SHRIMPLIN']  \n",
    "data.append(df)\n",
    "df = training_data0[training_data0['Well Name'] == 'NEWBY']  \n",
    "data.append(df)\n",
    "df = training_data0[training_data0['Well Name'] == 'Recruit F9']  \n",
    "data.append(df)\n",
    "Regime_3 = pd.concat(data, axis=0)\n",
    "print len(Regime_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the data into 2 parts:**\n",
    "\n",
    "from A We will make initial predictions\n",
    "\n",
    "from B we will make the final prediction(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 0:\n",
    "---------------------------------\n",
    "-Create predictions specifically for the most difficult facies\n",
    "\n",
    "-at this stage we focus on TP and FP only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________\n",
    "**training for facies 9 specifically**\n",
    "___________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df0 = test_data[test_data['Well Name'] == 'CRAWFORD']  \n",
    "# df1 = df0.drop(['Formation', 'Well Name', 'Depth'], axis=1)\n",
    "# df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "# blind=magic(df1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df0 = test_data[test_data['Well Name'] == blindwell] \n",
    "df1 = df0.drop(['Formation', 'Well Name', 'Depth'], axis=1)\n",
    "\n",
    "#df0 = training_data0[training_data0['Well Name'] == blindwell]  \n",
    "#df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "\n",
    "#\n",
    "df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "blind=magic(df1a)\n",
    "\n",
    "#features_blind = blind.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "features_blind = blind.drop(['Formation', 'Well Name', 'Depth'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio now:\n",
      "19.0434782609\n",
      "ratio after:\n",
      "6.34782608696\n",
      "kk is 3, nr of predictions for this regime is 0\n",
      "----------------------------------\n",
      "ratio now:\n",
      "19.0434782609\n",
      "ratio after:\n",
      "4.76086956522\n",
      "kk is 4, nr of predictions for this regime is 0\n",
      "----------------------------------\n",
      "ratio now:\n",
      "19.0434782609\n",
      "ratio after:\n",
      "3.80869565217\n",
      "kk is 5, nr of predictions for this regime is 0\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#============================================================\n",
    "df0=training_data0.dropna()\n",
    "df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "all1=magic(df1a)\n",
    "#X, y = make_balanced_binary(all1, 9,6)\n",
    "for kk in range(3,6):\n",
    "    X, y = make_balanced_binary(all1, 9,kk)\n",
    "#============================================================\n",
    "    correct_train=y\n",
    "\n",
    "    #clf = RandomForestClassifier(max_depth = 6, n_estimators=1600)\n",
    "    clf = RandomForestClassifier(max_depth = 6, n_estimators=800)\n",
    "    clf.fit(X,correct_train)\n",
    "\n",
    "    predicted_blind1 = clf.predict(features_blind)\n",
    "\n",
    "    predicted_regime9=predicted_blind1.copy()\n",
    "    print(\"kk is %d, nr of predictions for this regime is %d\" % (kk, sum(predicted_regime9)))\n",
    "    print \"----------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________\n",
    "**training for facies 1 specifically**\n",
    "________________________\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio now:\n",
      "11.4594594595\n",
      "ratio after:\n",
      "5.72972972973\n",
      "-------\n",
      "kk is 2, nr of predictions for this regime is 26\n",
      "----------------------------------\n",
      "ratio now:\n",
      "11.4594594595\n",
      "ratio after:\n",
      "3.81981981982\n",
      "-------\n",
      "kk is 3, nr of predictions for this regime is 7\n",
      "----------------------------------\n",
      "ratio now:\n",
      "11.4594594595\n",
      "ratio after:\n",
      "2.86486486486\n",
      "-------\n",
      "kk is 4, nr of predictions for this regime is 0\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#features_blind = blind.drop(['Formation', 'Well Name', 'Depth'], axis=1)\n",
    "\n",
    "#============================================================\n",
    "df0=training_data0.dropna()\n",
    "df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "all1=magic(df1a)\n",
    "\n",
    "#for kk in range(1,13):\n",
    "for kk in range(2,5): \n",
    "    X, y = make_balanced_binary(all1, 1,kk)\n",
    "    #============================================================\n",
    "\n",
    "    #=============================================\n",
    "    go_A=StandardScaler().fit_transform(X)\n",
    "    go_blind=StandardScaler().fit_transform(features_blind)\n",
    "    correct_train_A=binarify(y, 1)\n",
    "                                        \n",
    "\n",
    "    clf = linear_model.LogisticRegression()\n",
    "    clf.fit(go_A,correct_train_A)\n",
    "    predicted_blind1 = clf.predict(go_blind)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf.fit(go_A,correct_train_A)                                                  \n",
    "    predicted_blind2 = clf.predict(go_blind)\n",
    "\n",
    "    clf = svm.SVC(decision_function_shape='ovo')\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind3 = clf.predict(go_blind)\n",
    "\n",
    "    clf = svm.LinearSVC()\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind4 = clf.predict(go_blind)\n",
    "\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    predicted_blind=predicted_blind1+predicted_blind2+predicted_blind3+predicted_blind4\n",
    "    for ii in range(len(predicted_blind)):\n",
    "        if predicted_blind[ii] > 3:\n",
    "            predicted_blind[ii]=1\n",
    "        else:\n",
    "            predicted_blind[ii]=0 \n",
    "        \n",
    "    for ii in range(len(predicted_blind)):\n",
    "        if predicted_blind[ii] == 1 and predicted_blind[ii-1] == 0 and predicted_blind[ii+1] == 0:\n",
    "            predicted_blind[ii]=0\n",
    "        if predicted_blind[ii] == 1 and predicted_blind[ii-1] == 0 and predicted_blind[ii+2] == 0:\n",
    "            predicted_blind[ii]=0        \n",
    "        if predicted_blind[ii] == 1 and predicted_blind[ii-2] == 0 and predicted_blind[ii+1] == 0:\n",
    "            predicted_blind[ii]=0     \n",
    "    #####################################    \n",
    "\n",
    "    print \"-------\"\n",
    "    predicted_regime1=predicted_blind.copy()\n",
    "\n",
    "    #print(\"%c is my %s letter and my number %d number is %.5f\" % ('X', 'favorite', 1, .14))\n",
    " \n",
    "    print(\"kk is %d, nr of predictions for this regime is %d\" % (kk, sum(predicted_regime1)))\n",
    "    print \"----------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training for facies 5 specifically**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio now:\n",
      "13.8709677419\n",
      "ratio after:\n",
      "13.8709677419\n",
      "-------\n",
      "kk is 1, nr of predictions for this regime is 94\n",
      "----------------------------------\n",
      "ratio now:\n",
      "13.8709677419\n",
      "ratio after:\n",
      "6.93548387097\n",
      "-------\n",
      "kk is 2, nr of predictions for this regime is 38\n",
      "----------------------------------\n",
      "ratio now:\n",
      "13.8709677419\n",
      "ratio after:\n",
      "4.62365591398\n",
      "-------\n",
      "kk is 3, nr of predictions for this regime is 22\n",
      "----------------------------------\n",
      "ratio now:\n",
      "13.8709677419\n",
      "ratio after:\n",
      "3.46774193548\n",
      "-------\n",
      "kk is 4, nr of predictions for this regime is 18\n",
      "----------------------------------\n",
      "ratio now:\n",
      "13.8709677419\n",
      "ratio after:\n",
      "2.77419354839\n",
      "-------\n",
      "kk is 5, nr of predictions for this regime is 11\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#features_blind = blind.drop(['Formation', 'Well Name', 'Depth'], axis=1)\n",
    "\n",
    "#============================================================\n",
    "df0=training_data0.dropna()\n",
    "df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "all1=magic(df1a)\n",
    "for kk in range(1,6):\n",
    "#for kk in range(2,4): \n",
    "    X, y = make_balanced_binary(all1, 5,kk)\n",
    "    #X, y = make_balanced_binary(all1, 5,13)\n",
    "    #============================================================\n",
    "\n",
    "    go_A=StandardScaler().fit_transform(X)\n",
    "    go_blind=StandardScaler().fit_transform(features_blind)\n",
    "    correct_train_A=binarify(y, 1)\n",
    "    #=============================================                                        \n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=4,algorithm='brute')\n",
    "    clf.fit(go_A,correct_train_A)\n",
    "    predicted_blind1 = clf.predict(go_blind)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=5,leaf_size=10)\n",
    "    clf.fit(go_A,correct_train_A)                                                  \n",
    "    predicted_blind2 = clf.predict(go_blind)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind3 = clf.predict(go_blind)\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind4 = clf.predict(go_blind)\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind5 = clf.predict(go_blind)\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(go_A,correct_train_A)    \n",
    "    predicted_blind6 = clf.predict(go_blind)\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    predicted_blind=predicted_blind1+predicted_blind2+predicted_blind3+predicted_blind4+predicted_blind5+predicted_blind6\n",
    "    for ii in range(len(predicted_blind)):\n",
    "        if predicted_blind[ii] > 4:\n",
    "            predicted_blind[ii]=1\n",
    "        else:\n",
    "            predicted_blind[ii]=0 \n",
    "\n",
    "    print \"-------\"\n",
    "    predicted_regime5=predicted_blind.copy()\n",
    "    print(\"kk is %d, nr of predictions for this regime is %d\" % (kk, sum(predicted_regime5)))\n",
    "    print \"----------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**training for facies 7 specifically**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "15.9642857143\n",
      "-------\n",
      "kk is 2, nr of predictions for this regime is 65\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "10.6428571429\n",
      "-------\n",
      "kk is 3, nr of predictions for this regime is 54\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "7.98214285714\n",
      "-------\n",
      "kk is 4, nr of predictions for this regime is 45\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "6.38571428571\n",
      "-------\n",
      "kk is 5, nr of predictions for this regime is 24\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "5.32142857143\n",
      "-------\n",
      "kk is 6, nr of predictions for this regime is 22\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "4.5612244898\n",
      "-------\n",
      "kk is 7, nr of predictions for this regime is 16\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "3.99107142857\n",
      "-------\n",
      "kk is 8, nr of predictions for this regime is 14\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "3.54761904762\n",
      "-------\n",
      "kk is 9, nr of predictions for this regime is 9\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "3.19285714286\n",
      "-------\n",
      "kk is 10, nr of predictions for this regime is 9\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "2.9025974026\n",
      "-------\n",
      "kk is 11, nr of predictions for this regime is 10\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "2.66071428571\n",
      "-------\n",
      "kk is 12, nr of predictions for this regime is 7\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "2.45604395604\n",
      "-------\n",
      "kk is 13, nr of predictions for this regime is 6\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "2.2806122449\n",
      "-------\n",
      "kk is 14, nr of predictions for this regime is 11\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "2.12857142857\n",
      "-------\n",
      "kk is 15, nr of predictions for this regime is 8\n",
      "----------------------------------\n",
      "ratio now:\n",
      "31.9285714286\n",
      "ratio after:\n",
      "1.99553571429\n",
      "-------\n",
      "kk is 16, nr of predictions for this regime is 4\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#features_blind = blind.drop(['Formation', 'Well Name', 'Depth'], axis=1)\n",
    "\n",
    "#============================================================\n",
    "df0=training_data0.dropna()\n",
    "df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "all1=magic(df1a)\n",
    "for kk in range(2,17):\n",
    "    X, y = make_balanced_binary(all1, 7,kk)\n",
    "    #X, y = make_balanced_binary(all1, 7,13)\n",
    "    #============================================================\n",
    "\n",
    "    go_A=StandardScaler().fit_transform(X)\n",
    "    go_blind=StandardScaler().fit_transform(features_blind)\n",
    "    correct_train_A=binarify(y, 1)\n",
    "    #=============================================                                        \n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=4,algorithm='brute')\n",
    "    clf.fit(go_A,correct_train_A)\n",
    "    predicted_blind1 = clf.predict(go_blind)\n",
    "\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=5,leaf_size=10)\n",
    "    clf.fit(go_A,correct_train_A)                                                  \n",
    "    predicted_blind2 = clf.predict(go_blind)\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind3 = clf.predict(go_blind)\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind4 = clf.predict(go_blind)\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(go_A,correct_train_A)   \n",
    "    predicted_blind5 = clf.predict(go_blind)\n",
    "\n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf.fit(go_A,correct_train_A)    \n",
    "    predicted_blind6 = clf.predict(go_blind)\n",
    "\n",
    "\n",
    "    #####################################\n",
    "    predicted_blind=predicted_blind1+predicted_blind2+predicted_blind3+predicted_blind4+predicted_blind5+predicted_blind6\n",
    "    for ii in range(len(predicted_blind)):\n",
    "        if predicted_blind[ii] > 5:\n",
    "            predicted_blind[ii]=1\n",
    "        else:\n",
    "            predicted_blind[ii]=0 \n",
    "\n",
    "\n",
    "    #####################################    \n",
    "    print \"-------\"\n",
    "    predicted_regime7=predicted_blind.copy()\n",
    "    print(\"kk is %d, nr of predictions for this regime is %d\" % (kk, sum(predicted_regime7)))\n",
    "    print \"----------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PHASE Ib \n",
    "======================================\n",
    "**PREPARE THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_data(Regime_1, Regime_2, Regime_3, training_data0, w1, w2,w3):\n",
    "    df0=Regime_1.dropna()\n",
    "    df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "    df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "    df2a=magic(df1a)\n",
    "    feature_names0 = ['GR', 'ILD_log10','DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS', 'PHIND_1', 'PHIND_2']\n",
    "    X0 = df2a[feature_names0].values\n",
    "    df2a=(df1a)\n",
    "    y=df2a['Facies'].values\n",
    "    feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "    X1 = df2a[feature_names].values\n",
    "    well = df2a['Well Name'].values\n",
    "    depth = df2a['Depth'].values\n",
    "    X2, padded_rows = augment_features(X1, well, depth)\n",
    "    Xtot_train=np.column_stack((X0,X2))\n",
    "    regime1A_train, regime1B_train, regime1A_test, regime1B_test = train_test_split(Xtot_train, y, test_size=w1, random_state=42)\n",
    "\n",
    "    df0=Regime_2.dropna()\n",
    "    df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "    df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "    df2a=magic(df1a)\n",
    "    feature_names0 = ['GR', 'ILD_log10','DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS', 'PHIND_1', 'PHIND_2']\n",
    "    X0 = df2a[feature_names0].values\n",
    "    df2a=(df1a)\n",
    "    y=df2a['Facies'].values\n",
    "    feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "    X1 = df2a[feature_names].values\n",
    "    well = df2a['Well Name'].values\n",
    "    depth = df2a['Depth'].values\n",
    "    X2, padded_rows = augment_features(X1, well, depth)\n",
    "    Xtot_train=np.column_stack((X0,X2))\n",
    "    regime2A_train, regime2B_train, regime2A_test, regime2B_test = train_test_split(Xtot_train, y, test_size=w2, random_state=42)\n",
    "\n",
    "\n",
    "    df0=Regime_3.dropna()\n",
    "    df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "    df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "    df2a=magic(df1a)\n",
    "    feature_names0 = ['GR', 'ILD_log10','DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS', 'PHIND_1', 'PHIND_2']\n",
    "    X0 = df2a[feature_names0].values\n",
    "    df2a=(df1a)\n",
    "    y=df2a['Facies'].values\n",
    "    feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "    X1 = df2a[feature_names].values\n",
    "    well = df2a['Well Name'].values\n",
    "    depth = df2a['Depth'].values\n",
    "    X2, padded_rows = augment_features(X1, well, depth)\n",
    "    Xtot_train=np.column_stack((X0,X2))\n",
    "    regime3A_train, regime3B_train, regime3A_test, regime3B_test = train_test_split(Xtot_train, y, test_size=w3, random_state=42)\n",
    "\n",
    "\n",
    "    #df0 = training_data0[training_data0['Well Name'] == blindwell]\n",
    "    #df1 = df0.drop(['Formation', 'Well Name', 'Depth','Facies'], axis=1)\n",
    "    df0 = test_data[test_data['Well Name'] == blindwell] \n",
    "    df1 = df0.drop(['Formation', 'Well Name', 'Depth'], axis=1)\n",
    "    df1a=df0[(np.abs(stats.zscore(df1))<8).all(axis=1)]\n",
    "    df2a=magic(df1a)\n",
    "    #df2a=df1a\n",
    "    X0blind = df2a[feature_names0].values\n",
    "\n",
    "    blind=df1a\n",
    "    #correct_facies_labels = blind['Facies'].values\n",
    "    feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "    X1 = blind[feature_names].values\n",
    "    well = blind['Well Name'].values\n",
    "    depth = blind['Depth'].values\n",
    "    X2blind,  padded_rows = augment_features(X1, well, depth)\n",
    "\n",
    "    features_blind=np.column_stack((X0blind,X2blind))\n",
    "#=======================================================\n",
    "    main_regime=regime2A_train\n",
    "    other1=regime1A_train\n",
    "    other2=regime3A_train\n",
    "\n",
    "    main_test=regime2A_test\n",
    "    other1_test=regime1A_test\n",
    "    other2_test=regime3A_test\n",
    "\n",
    "    go_B=np.concatenate((regime1B_train, regime2B_train, regime3B_train))\n",
    "    correctB=np.concatenate((regime1B_test, regime2B_test, regime3B_test))\n",
    "#     #===================================================\n",
    "    train1= np.concatenate((main_regime, other1, other2))\n",
    "    correctA1=np.concatenate((main_test, other1_test, other2_test))\n",
    "#     #=================================================== \n",
    "#     train2= np.concatenate((main_regime, other2))\n",
    "#     correctA2=np.concatenate((main_test, other2_test))\n",
    "#     #===================================================\n",
    "\n",
    "    #===================================================\n",
    "    #train1=main_regime\n",
    "    #correctA1=main_test\n",
    "    train2=other1\n",
    "    correctA2=other1_test   \n",
    "    train3=other2\n",
    "    correctA3=other2_test   \n",
    "\n",
    "    return train1, train2, train3, correctA1, correctA2, correctA3, correctB, go_B, features_blind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREPARE THE DATA FOR SERIAL MODELLING**\n",
    "\n",
    "This could be done smarter but basically manual at this point\n",
    "\n",
    "selecting bias towards the REGIME the blind data has been classified as\n",
    "\n",
    "\n",
    "for CHURCHMAN BIBLE  this is regime 3\n",
    "\n",
    "For CRAWFORD this is regime 1\n",
    "\n",
    "For STUART this is regime 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create several predictions, varying the dataset and the technique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_phaseI(train1,train2,train3,correctA1,correctA2,correctA3,correctB, go_B, features_blind):    \n",
    "    pred_array=0*correctB\n",
    "    pred_blind=np.zeros(len(features_blind))\n",
    "\n",
    "    print \"rf1\"\n",
    "    clf = RandomForestClassifier(max_depth = 5, n_estimators=600, random_state=1)\n",
    "    pred_array, pred_blind=phaseI_model(train1, correctA1, go_B, clf, pred_array, pred_blind, features_blind)\n",
    "    clf = RandomForestClassifier(max_depth = 15, n_estimators=3000, random_state=1)\n",
    "    pred_array, pred_blind=phaseI_model(train1, correctA1, go_B, clf, pred_array, pred_blind, features_blind)    \n",
    "#     pred_array, pred_blind=phaseI_model(train2, correctA2, go_B, clf, pred_array, pred_blind, features_blind)\n",
    "#     pred_array, pred_blind=phaseI_model(train3, correctA3, go_B, clf, pred_array, pred_blind, features_blind)\n",
    "    clf = RandomForestClassifier(n_estimators=600, max_depth = 15, criterion='entropy',\n",
    "                                 max_features=10, min_samples_split=25, min_samples_leaf=5,\n",
    "                                 class_weight='balanced', random_state=1)\n",
    "    pred_array, pred_blind=phaseI_model(train1, correctA1, go_B, clf, pred_array, pred_blind, features_blind)\n",
    "    #pred_array, pred_blind=phaseI_model(train2, correctA2, go_B, clf, pred_array, pred_blind, features_blind)\n",
    "    #pred_array, pred_blind=phaseI_model(train3, correctA3, go_B, clf, pred_array, pred_blind, features_blind)\n",
    "    return pred_array, pred_blind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase II:\n",
    "---------------------------------------------\n",
    "Stacking the predictions from phase Ib. \n",
    "New predictions from data B\n",
    "\n",
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First prediction of B data without Phase I input:**\n",
    "**Then add the initial predictions as features:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make a new prediction, with the best model on the full dataset B:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weigths w1 ,w2 ,w3 are the magic parameters representing the difference between glory and hopeless failure\n",
    "\n",
    "The are optimized for Alex and Luke (Crawford) and Kimzey and Nolan (Stuart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running phase I:\n",
      "rf1\n",
      "prediction phase II:\n",
      "prediction phase II-stacked:\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "#KOPIJ DIT!!!\n",
    "\n",
    "#correct_facies_labels = blind['Facies'].values\n",
    "\n",
    "w1= 0.95  \n",
    "w2= 0.75  \n",
    "w3= 0.5  \n",
    "\n",
    "   \n",
    "\n",
    "# For each set of parameters\n",
    "score_param_phaseI = []\n",
    "score_param_phaseII = []\n",
    "\n",
    "train1, train2, train3, correctA1, correctA2, correctA3, correctB, go_B, features_blind=prepare_data(Regime_1, Regime_2, Regime_3, training_data1, w1, w2,w3)\n",
    "#train1, train2, train3, correctA1, correctA2, correctA3, correctB, go_B, features_blind=prepare_data_NOPE(Regime_1, Regime_2, Regime_3, training_data1, w1, w2,w3)\n",
    "\n",
    "print \"running phase I:\"\n",
    "pred_array, pred_blind = run_phaseI(train1,train2,train3,correctA1,correctA2, correctA3, correctB, go_B, features_blind)\n",
    "print \"prediction phase II:\"\n",
    "clf = RandomForestClassifier(max_depth = 8, n_estimators=3000, max_features=10, criterion='entropy',class_weight='balanced')\n",
    "#clf = RandomForestClassifier(max_depth = 5, n_estimators=600, max_features=10, criterion='entropy',class_weight='balanced')\n",
    "#clf = RandomForestClassifier(n_estimators=1200, max_depth = 15, criterion='entropy',\n",
    "#                         max_features=10, min_samples_split=25, min_samples_leaf=5,\n",
    "#                         class_weight='balanced', random_state=1)\n",
    "\n",
    "clf.fit(go_B,correctB)\n",
    "predicted_blind_PHASE_I = clf.predict(features_blind)\n",
    "\n",
    "print \"prediction phase II-stacked:\"\n",
    "pa=pred_array[:len(pred_array)-1]\n",
    "go_B_PHASE_II=np.concatenate((pa, go_B.transpose())).transpose()\n",
    "pa1=np.median(pa,axis=0)\n",
    "go_B_PHASE_II=np.column_stack((go_B_PHASE_II,pa1))\n",
    "#print go_B_PHASE_II.shape\n",
    "feat=pred_blind[:len(pred_blind)-1]\n",
    "features_blind_PHASE_II=np.concatenate((feat, features_blind.transpose())).transpose()\n",
    "feat1=np.median(feat,axis=0)\n",
    "features_blind_PHASE_II=np.column_stack((features_blind_PHASE_II,feat1))\n",
    "\n",
    "#second pred\n",
    "clf.fit(go_B_PHASE_II,correctB)\n",
    "predicted_blind_PHASE_II = clf.predict(features_blind_PHASE_II)\n",
    "#out_f1=metrics.f1_score(correct_facies_labels, predicted_blind_PHASE_I, average = 'micro')\n",
    "#print \" f1 score on phase I:\"\n",
    "#print out_f1    \n",
    "#score_param_phaseI.append(out_f1)\n",
    "#out_f1=metrics.f1_score(correct_facies_labels, predicted_blind_PHASE_II, average = 'micro')\n",
    "#score_param_phaseII.append(out_f1)\n",
    "#print \" f1 score on phase II:\"\n",
    "#print out_f1\n",
    "print \"finished\"\n",
    "    \n",
    "# Best set of parameters\n",
    "#best_idx = np.argmax(score_param_phaseII)\n",
    "#param_best = param_grid[best_idx]\n",
    "#score_best = score_param_phaseII[best_idx]\n",
    "#print param_best\n",
    "#print score_best\n",
    "#c = csv.writer(open(\"score4.csv\", \"wb\"))\n",
    "#c.writerow([param_best])\n",
    "#c.writerow([score_best])\n",
    "#print('\\nBest F1 score = %.3f %s' % (score_best, param_best))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best F1 score = 0.633 {'w3': 0.55, 'w2': 0.95, 'w1': 0.55}\n"
     ]
    }
   ],
   "source": [
    "best_idx = np.argmax(score_param_phaseII)\n",
    "param_best = param_grid[best_idx]\n",
    "score_best = score_param_phaseII[best_idx]\n",
    "print('\\nBest F1 score = %.3f %s' % (score_best, param_best))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Permute facies based on earlier predictions**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "4\n",
      "0\n",
      "0\n",
      "values changed:\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(sum(predicted_regime5))\n",
    "predicted_blind_PHASE_IIa=permute_facies_nr(predicted_regime5, predicted_blind_PHASE_II, 5)\n",
    "print(sum(predicted_regime7))\n",
    "predicted_blind_PHASE_IIb=permute_facies_nr(predicted_regime7, predicted_blind_PHASE_IIa, 7)\n",
    "print(sum(predicted_regime1))\n",
    "predicted_blind_PHASE_IIc=permute_facies_nr(predicted_regime1, predicted_blind_PHASE_IIb, 1)\n",
    "print(sum(predicted_regime9))\n",
    "predicted_blind_PHASE_III=permute_facies_nr(predicted_regime9, predicted_blind_PHASE_IIc, 9)\n",
    "\n",
    "\n",
    "print \"values changed:\"\n",
    "\n",
    "print len(predicted_blind_PHASE_II)-np.count_nonzero(predicted_blind_PHASE_III==predicted_blind_PHASE_II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from phase I to phase II, changed this many values:\n",
      "83\n",
      "Phase I: f1 score on the prediction of blind:\n",
      "0.620390455531\n",
      "Phase II:f1 score on the prediction of blind:\n",
      "0.659436008677\n",
      "from phase II to phase III, changed this many values:\n",
      "11\n",
      "Phase III:final f1 score on the prediction of blind:\n",
      "0.663774403471\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print \"from phase I to phase II, changed this many values:\"\n",
    "# print len(predicted_blind_PHASE_II)-np.count_nonzero(predicted_blind_PHASE_II==predicted_blind_PHASE_I)\n",
    "# out_f1=metrics.f1_score(correct_facies_labels, predicted_blind_PHASE_I, average = 'micro')\n",
    "# print \"Phase I: f1 score on the prediction of blind:\"\n",
    "# print out_f1\n",
    "\n",
    "# out_f1=metrics.f1_score(correct_facies_labels, predicted_blind_PHASE_II, average = 'micro')\n",
    "# print \"Phase II:f1 score on the prediction of blind:\"\n",
    "# print out_f1\n",
    "\n",
    "\n",
    "# print \"from phase II to phase III, changed this many values:\"\n",
    "# print len(predicted_blind_PHASE_II)-np.count_nonzero(predicted_blind_PHASE_III==predicted_blind_PHASE_II)\n",
    "# out_f1=metrics.f1_score(correct_facies_labels, predicted_blind_PHASE_III, average = 'micro')\n",
    "# print \"Phase III:final f1 score on the prediction of blind:\"\n",
    "# print out_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(2, 71), (3, 21), (4, 34), (5, 12), (6, 67), (7, 79), (8, 72)])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=Counter(predicted_blind_PHASE_I)\n",
    "y = OrderedDict(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(1, 1),\n",
       "             (2, 68),\n",
       "             (3, 23),\n",
       "             (4, 41),\n",
       "             (5, 5),\n",
       "             (6, 63),\n",
       "             (7, 71),\n",
       "             (8, 84)])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=Counter(predicted_blind_PHASE_II)\n",
    "y = OrderedDict(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(1, 1),\n",
       "             (2, 68),\n",
       "             (3, 23),\n",
       "             (4, 41),\n",
       "             (5, 15),\n",
       "             (6, 60),\n",
       "             (7, 67),\n",
       "             (8, 81)])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=Counter(predicted_blind_PHASE_III)\n",
    "y = OrderedDict(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 5, 4, 4, 4, 4, 6,\n",
       "       6, 8, 8, 8, 8, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 4,\n",
       "       4, 4, 4, 4, 4, 7, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 4, 4, 8, 8, 8, 6, 8,\n",
       "       4, 4, 5, 5, 5, 6, 6, 8, 8, 3, 3, 8, 8, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "       6, 5, 8, 8, 8, 6, 6, 8, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 3, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 8, 8, 8, 8, 4, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2,\n",
       "       2, 2, 2, 2, 3, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 6, 6, 5, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 8, 8, 8, 8,\n",
       "       8, 8, 8, 8, 8, 6, 6, 6, 6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 3, 3, 8, 8, 8, 8, 8, 5, 8, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 7, 7, 7, 7, 7, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "       7, 7, 7, 5, 5, 7, 7, 7, 8, 8, 8, 6, 6, 6, 6, 4, 3, 3, 3, 3, 3, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_CRAWFORD=predicted_blind_PHASE_III\n",
    "predicted_CRAWFORD"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
